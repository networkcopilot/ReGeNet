{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6d2kX0NNKrE",
        "outputId": "08fb53ca-7dd6-4c3e-c9a4-c994833a3bda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import time\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import math\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "# OpenAI API Key\n",
        "api_key = userdata.get('api_key')#need to add the key to the 'secrets section'\n",
        "client = OpenAI(api_key = api_key)"
      ],
      "metadata": {
        "id": "Q55HSH1tNPlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "drKqwUVVNRkF",
        "outputId": "c5c79bd0-acd9-468f-83a6-bc0b76975ec4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementation Evaluation"
      ],
      "metadata": {
        "id": "-ErzdxGqNeb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The prompt for this phase's evaluation:**\n",
        "> Hello Networking Intent Implementation Evaluator, I added here {file_number} files: '{input_configuration_file_name}' which is the initial network component configuration file, '{input_textual_topology_name}' which is the initial topology description, an intent file named 'intent.txt', and {len(files)} updated files which shows the changes after intent implementation named {files_names}.\n",
        "Here are the scoring keys for your evaluation:\n",
        "{scoring_keys}\n",
        "Given the these files, and the scoring keys, assign a grade to the intent implementation according to your instructions, and briefly explain your scoring for each key.,\n"
      ],
      "metadata": {
        "id": "170LuQXQNgxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ### assistant creation\n",
        "# temp = 0.0\n",
        "# selected_model = 'gpt-4.1-mini'\n",
        "\n",
        "# client.beta.assistants.create(\n",
        "#         model= selected_model,\n",
        "#         name = \"Intent Implementation Evaluator Assistant\",\n",
        "#         description=\"A Networking Intent Implementation Evaluator\",\n",
        "#         instructions = 'You are a helpful networking intent implementation evaluator assistant. Your role as an evaluator is to assess the accuracy of network intent implementation by grading it. You will review the provided files:\n",
        "# 1. Initial Network Components Configuration – The starting configuration of the network devices.\n",
        "# 2. Initial Topology Description – The original structure of the network.\n",
        "# 3. Textual Intent – A detailed description of the desired changes or outcomes for the network's configuration.\n",
        "# 4. Updated Network Components Configuration files or topology files – Several files which show the changes made to the topology or to the configuration based on the intent. The number of those files can be one or more.\n",
        "\n",
        "# Evaluation Process:\n",
        "# 1. Understand the Intent: Carefully analyze the textual intent to fully grasp the desired changes or outcomes.\n",
        "# 2. Analyze Initial Files: Examine the initial network configuration and topology to understand the starting state and context.\n",
        "# 3. Evaluate Updated Files: Compare the updated configurations against the intent to determine if the desired changes have been accurately implemented.\n",
        "\n",
        "# Scoring and Grading:\n",
        "# - Scoring Keys: You will be provided with specific scoring keys by the user to guide your evaluation. Each of those keys specifies a condition for noncompliance with the intent implementation, and the point deductions for this specific noncompliance.\n",
        "# \t- Points should only be deducted if the updated file complies with the key’s condition (i.e., if the issue described in the key is present in the implementation).\n",
        "# \t- Partial deductions are allowed for partial compliance or minor issues related to a key.\n",
        "# - Grading:\n",
        "# \t- Start with a grade of 100.\n",
        "# \t- If the implementation meets the noncompliance criteria described in a key, subtract the corresponding points from the grade.\n",
        "# \t- Ensure deductions align strictly with the provided scoring keys and do not penalize issues outside their scope.\n",
        "\n",
        "# Response Guidelines:\n",
        "# - Provide the final grade as a numeric value (0–100).\n",
        "# - Adhere strictly to these instructions and the provided scoring keys during the evaluation process.\n",
        "# - In your response, respond **only** with a single JSON object that conforms to the following schema:\n",
        "#  ```json\n",
        "#     {\n",
        "#         \"explanation\":<your reasoning process and explanations alongside corresponding point deductions>,\n",
        "#         \"final_grade\": <numerical grade>,\n",
        "\n",
        "#     }\n",
        "#     ```',\n",
        "#         tools=[{\"type\": \"file_search\"}],\n",
        "#         temperature = temp,\n",
        "#         response_format={\n",
        "#             \"type\": \"json_schema\",\n",
        "#             \"json_schema\": {\n",
        "#                 \"name\": \"evaluator_response\",\n",
        "#                 \"schema\":{\n",
        "#                     \"type\": \"object\",\n",
        "#                     \"properties\":{\n",
        "#                         \"explanation\": {\"type\": \"string\"},\n",
        "#                         \"final_grade\": {\"type\": \"number\", \"minimum\": 0,\"maximum\": 100},\n",
        "#                     },\n",
        "#                     \"required\": [\"explanation\", \"final_grade\"],\n",
        "#                     \"additionalProperties\": False\n",
        "#                 },\n",
        "#                 \"strict\": True\n",
        "#             },\n",
        "#         }\n",
        "#     )\n",
        "\n"
      ],
      "metadata": {
        "id": "elFT6F4dNjjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#----------- Run Related Values----------------\n",
        "eval_assistant_temp = 0\n",
        "model = \"gpt-4.1-mini\"\n",
        "run_number = 1\n",
        "#----------------------------------------------------------------"
      ],
      "metadata": {
        "id": "QK4STNI2NnjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_eval_thread(intent, input_config, input_topology, input_topology_name, updated_files_path, scoring_keys_address):\n",
        "\n",
        "  input_configuration_file_name = 'Total_Configs.txt'\n",
        "  input_textual_topology_name = f'{input_topology_name}.json'\n",
        "\n",
        "  files = [f for f in os.listdir(updated_files_path) if os.path.isfile(os.path.join(updated_files_path, f))]\n",
        "\n",
        "  updated_files = []\n",
        "  for file in files:\n",
        "    file_path = os.path.join(updated_files_path, file)\n",
        "    assistant_file = client.files.create(file=open(file_path, \"rb\"), purpose='assistants')\n",
        "    updated_files.append((file, assistant_file.id))\n",
        "  input_config_file = client.files.create(file=open(input_config, \"rb\"), purpose='assistants')\n",
        "  input_topology_file = client.files.create(file=open(input_topology, \"rb\"), purpose='assistants')\n",
        "  intent_file = client.files.create(file=open(intent, \"rb\"), purpose='assistants')\n",
        "\n",
        "  with open( scoring_keys_address, 'r') as file:\n",
        "    scoring_keys = file.read()\n",
        "\n",
        "  attachments = [\n",
        "        {\"file_id\": input_config_file.id, \"tools\": [{\"type\": \"file_search\"}]},\n",
        "        {\"file_id\": input_topology_file.id, \"tools\": [{\"type\": \"file_search\"}]},\n",
        "        {\"file_id\": intent_file.id, \"tools\": [{\"type\": \"file_search\"}]}]\n",
        "\n",
        "  for file in updated_files:\n",
        "    attachments.append({\"file_id\": file[1], \"tools\": [{\"type\": \"file_search\"}]})\n",
        "\n",
        "  file_number = 3+len(files)\n",
        "\n",
        "  if len(files) == 1:\n",
        "    updated_file_message = f\"and an updated file which shows the changes after intent implementation named {updated_files[0][0]}\"\n",
        "  else:\n",
        "    files_names = \"\"\n",
        "    for i in range(len(files)-1):\n",
        "      files_names = files_names+f\"{updated_files[i][0]}, \"\n",
        "    files_names = files_names+f\" and {updated_files[-1][0]}\"\n",
        "    updated_file_message = f\"and {len(files)} updated files which shows the changes after intent implementation named {files_names}\"\n",
        "\n",
        "\n",
        "  message_thread = client.beta.threads.create(\n",
        "    messages=[\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": f\"\"\"Hello Networking Intent Implementation Evaluator, I added here {file_number} files: '{input_configuration_file_name}' which is the initial network component configuration file, '{input_textual_topology_name}' which is the initial topology description, an intent file named 'intent.txt', {updated_file_message}.\n",
        "Here are the scoring keys for your evaluation:\n",
        "{scoring_keys}\n",
        "Given the these files, and the scoring keys, assign a grade to the intent implementation according to your instructions, and briefly explain your scoring for each key.\"\"\",\n",
        "        #----------------- ADDITIONAL LINES FOR NEW API CODE-----------------------------\n",
        "        \"attachments\": attachments,\n",
        "        #-----------------------------------------------------------------------------------\n",
        "      },\n",
        "    ]\n",
        "  )\n",
        "  return message_thread"
      ],
      "metadata": {
        "id": "icSJo5s2NovJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "folders =[\"Adding_DMZ\", \"Adding_DRA\", \"Adding_Local_PCs\",\"Internet_Connectivity\", \"Adding_Communication_Servers\",\"Role_Based_CLI_Access\",\"Time_Based_Access_List\", \"Transparent_IOS_Firewall\", \"Basic_Zone_Based_Firewall\",\"IP_Traffic_Export\"]\n",
        "input_config_directory = \"path\"\n",
        "updated_specs_directory = f\"path/{model}\"\n",
        "scoring_keys_directory = \"path\"\n",
        "\n",
        "results_folder = f\"path/{model}\"\n",
        "\n",
        "os.makedirs(results_folder, exist_ok=True)\n",
        "if not os.path.isfile(f\"/path/{model}/Run{run_number}_tracking.csv\"):\n",
        "  itterations_file = pd.read_csv(\"path/Eval_result_template.csv\")\n",
        "else:\n",
        "  itterations_file = pd.read_csv(f\"path/{model}/Run{run_number}_tracking.csv\")\n",
        "\n",
        "for index, row in tqdm(itterations_file.iterrows(), total=len(itterations_file)):\n",
        "  if row[\"Phase4_Eval\"]==\"V\": #done\n",
        "    continue\n",
        "\n",
        "  platform = row[\"Platform\"]\n",
        "  diagram_type = row[\"Diagram_Type\"]\n",
        "  scenario = row[\"Scenario\"]\n",
        "  # skip if result files do not exist\n",
        "\n",
        "\n",
        "  if not os.path.exists(f\"path/Implementation_results/{model}/{platform}/{diagram_type}/(Run{run_number})/{scenario}/final_results\"):\n",
        "    print(f\"path/{model}/{platform}/{diagram_type}/(Run{run_number})/{scenario}/final_results\")\n",
        "    continue\n",
        "\n",
        "  output_dir_address = os.path.join(results_folder, platform, diagram_type,f\"(run{run_number})\")\n",
        "  os.makedirs(output_dir_address, exist_ok=True)\n",
        "  output_file_address = os.path.join(output_dir_address, f\"{scenario}_results.txt\")\n",
        "\n",
        "  output_file = open(output_file_address, \"a\")\n",
        "\n",
        "  input_config_scenario_folder = os.path.join(input_config_directory, scenario)\n",
        "  input_config_file_address = os.path.join(input_config_scenario_folder,\"Total_Configs.txt\")\n",
        "  intent_file_address = os.path.join(input_config_scenario_folder,\"intent.txt\")\n",
        "  updated_specs_scenario_directory = os.path.join(updated_specs_directory, platform, diagram_type,f\"(Run{run_number})\", scenario)\n",
        "  scoring_keys_address = os.path.join(scoring_keys_directory, f\"{scenario}.txt\")\n",
        "\n",
        "  updated_spec_run_directory = os.path.join(updated_specs_scenario_directory,\"final_results\")\n",
        "\n",
        "   ## different ground truth representations - only four types are needed to cover the 9 combinations)\n",
        "  if platform == \"PowerPoint\":\n",
        "     if diagram_type == \"No_Labels_On_Edges\": #(pp: no_lables)\n",
        "       gt_file_path = f\"path/No_Labels_On_Edges/{scenario}.json\"\n",
        "     else: #(pp: normal,messy)\n",
        "       gt_file_path = f\"path/PowerPoint/Normal/{scenario}.json\"\n",
        "  else:\n",
        "    if diagram_type == \"No_Labels_On_Edges\": #(no_labels:gns,paper)\n",
        "      gt_file_path = f\"path/No_Labels_On_Edges/{scenario}.json\"\n",
        "    else: #(normal: gns,paper, messy: gns,paper)\n",
        "      gt_file_path = f\"path/Normal/{scenario}.json\"\n",
        "\n",
        "\n",
        "\n",
        "  message_thread = create_eval_thread(intent_file_address, input_config_file_address, gt_file_path, scenario, updated_spec_run_directory, scoring_keys_address)\n",
        "  thread_id = message_thread.id\n",
        "  # run the thread\n",
        "  run = client.beta.threads.runs.create_and_poll(\n",
        "    temperature = eval_assistant_temp,\n",
        "    thread_id = thread_id,\n",
        "    assistant_id = assist_num)\n",
        "\n",
        "  print(f\"\\n-----------[PLATFORM: {platform}, DIAGRAM TYPE: {diagram_type}, SCENARIO: {scenario}]--------------\\n\")\n",
        "  # output_file.write(f\"\\n--------- RUN:{run_number}]-------------------\\n\")\n",
        "  if run.status == 'completed':\n",
        "    messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
        "    for message in messages.data:\n",
        "      if message.role == \"assistant\":\n",
        "        messg_id = message.id\n",
        "        value = message.content[0].text.value\n",
        "        output_file.write(f\"{value}\\n\")\n",
        "        print(value)\n",
        "  output_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzyc35lANqK4",
        "outputId": "765c6924-55c0-4704-d008-b897831ef43e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/90 [00:00<?, ?it/s]/tmp/ipython-input-1858307746.py:47: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
            "  message_thread = client.beta.threads.create(\n",
            "/tmp/ipython-input-276507296.py:82: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
            "  run = client.beta.threads.runs.create_and_poll(\n",
            "/tmp/ipython-input-276507296.py:90: DeprecationWarning: The Assistants API is deprecated in favor of the Responses API\n",
            "  messages = client.beta.threads.messages.list(thread_id=thread_id)\n",
            "100%|██████████| 90/90 [00:19<00:00,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-----------[PLATFORM: Paper_Sketches, DIAGRAM TYPE: Normal, SCENARIO: IP_Traffic_Export]--------------\n",
            "\n",
            "{\"explanation\":\"1. The next hop of router Ditto was set correctly in the route-map EXPORT_TO_IDS with 'set ip next-hop 192.168.24.4' for both HTTP and EIGRP traffic, so no deduction here.\\n\\n2. The interfaces on Ditto are correctly defined with IP addresses matching the initial configuration: FastEthernet0/0 (192.168.12.2), FastEthernet1/0 (192.168.23.2), and FastEthernet2/0 (192.168.24.2). IDS interface FastEthernet0/0 is also correctly configured with 192.168.24.4. No interfaces are missing or wrongly defined, so no deduction.\\n\\n3. The ACL named HTTP_TRAFFIC on Ditto permits TCP traffic from hosts 192.168.12.1 and 192.168.23.3 to 192.168.24.4 on port 80 (HTTP). The route-map EXPORT_TO_IDS matches this ACL and also matches protocol eigrp, setting the next hop to 192.168.24.4. This correctly allows EIGRP and HTTP traffic to be exported to IDS. No issues found here.\\n\\n4. The route-map EXPORT_TO_IDS is applied as an ip policy route-map on all three Ditto interfaces (FastEthernet0/0, FastEthernet1/0, FastEthernet2/0). This is correct and complete, so no deduction.\\n\\n5. The ACL HTTP_TRAFFIC only permits specific HTTP traffic from Repo (192.168.12.1) and Forge (192.168.23.3) to IDS (192.168.24.4) on port 80. No unnecessary traffic types are included in the ACL. No deduction.\\n\\n6. The correct router (Ditto) was configured for exporting traffic to IDS. IDS configuration is minimal but correct for its role. No wrong router configuration detected.\\n\\nOverall, the implementation fully complies with the intent and scoring keys. No point deductions are warranted.\",\"final_grade\":100}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting eval grades\n"
      ],
      "metadata": {
        "id": "3FdcfrBUNtbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results_folder = f\"path/{model}\"\n",
        "os.makedirs(results_folder, exist_ok=True)\n",
        "if not os.path.isfile(f\"path/{model}/Run{run_number}_tracking.csv\"):\n",
        "  itterations_file = pd.read_csv(\"path/Eval_result_template.csv\")\n",
        "else:\n",
        "  itterations_file = pd.read_csv(f\"path/{model}/Run{run_number}_tracking.csv\")\n",
        "\n",
        "for index, row in itterations_file.iterrows():\n",
        "  if row[\"Phase4_Eval\"] == \"V\": #done\n",
        "    continue\n",
        "\n",
        "  platform = row[\"Platform\"]\n",
        "  diagram_type = row[\"Diagram_Type\"]\n",
        "  scenario = row[\"Scenario\"]\n",
        "\n",
        "  result_dir_address = os.path.join(results_folder, platform, diagram_type,f\"(run{run_number})\",f\"{scenario}_results.txt\")\n",
        "  if not os.path.exists(result_dir_address):\n",
        "    continue\n",
        "  with open(result_dir_address, \"r\") as f:\n",
        "    data_str = f.read()\n",
        "  try:\n",
        "    result = json.loads(data_str)\n",
        "    row['Phase4_Eval'] = 'V'\n",
        "    row['Phase4 Grade'] = result['final_grade']\n",
        "  except:\n",
        "    row['Phase4_Eval'] = 'X'\n",
        "  itterations_file.iloc[index] = row\n",
        "itterations_file.to_csv(f\"path/{model}/Run{run_number}_tracking.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "AiJfcPFINwQl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}